{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# IMPORTS",
   "id": "e0a2bf1097997ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import json\n",
    "import re\n",
    "\n",
    "import faiss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import importlib\n",
    "import utils\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from utils import count_words\n",
    "\n",
    "importlib.reload(utils)"
   ],
   "id": "e84952dc6d5fa12b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATA CLEANING",
   "id": "4d55d657b3864e3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Instantiate variables",
   "id": "e1c5c782a170ed5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- CONFIGURATION ---\n",
    "INPUT_CSV_MSG = \"DATA/mod_progress_messages.csv\"\n",
    "OUTPUT_CSV_MSG = \"DATA/mod_progress_messages_clean.csv\"\n",
    "INPUT_CSV_PROGRESS = \"DATA/mod_progress.csv\"\n",
    "OUTPUT_CSV_PROGRESS = \"DATA/mod_progress_clean.csv\"\n",
    "\n",
    "OUTPUT_JSON = \"DATA/progress_dataset_dedup.json\"\n",
    "SIMILARITY_THRESHOLD = 0.95  # Cosine similarity above which we treat messages as duplicates\n",
    "\n",
    "INDEX_PATH = \"RAG/faiss_index\"\n",
    "METADATA_PATH = \"RAG/rag_metadata\"\n",
    "EMBEDDING_MODELS = [\n",
    "    \"all-MiniLM-L6-v2\",\n",
    "    # \"nomic-ai/nomic-embed-text-v1.5\",\n",
    "    \"multi-qa-distilbert-cos-v1\",\n",
    "    \"multi-qa-mpnet-base-dot-v1\"\n",
    "]"
   ],
   "id": "d6a5d645acd0808b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create cleaned csv files from the original ones",
   "id": "2123ad8c2988e480"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- CLEANING FUNCTION ---\n",
    "def clean_html(raw_html):\n",
    "    # Remove HTML tags and decode HTML entities\n",
    "    soup = BeautifulSoup(str(raw_html), \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Remove links\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# If clean files already exist, skip cleaning\n",
    "try:\n",
    "    df_msgs = pd.read_csv(OUTPUT_CSV_MSG)\n",
    "    print(\"Messages CSV already exists. Skipping cleaning.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Messages CSV not found. Proceeding with cleaning.\")\n",
    "    # --- LOAD & CLEAN ---\n",
    "    df_msgs = pd.read_csv(INPUT_CSV_MSG)\n",
    "\n",
    "    # Apply cleaning\n",
    "    df_msgs[\"clean_text\"] = df_msgs[\"tText\"].apply(clean_html)\n",
    "\n",
    "    # Filter out empty or too short messages\n",
    "    df_msgs = df_msgs[df_msgs[\"clean_text\"].str.len() > 10]\n",
    "\n",
    "    df_msgs = df_msgs.drop(columns=[\"tText\"])\n",
    "\n",
    "    # --- PREP ---\n",
    "    df_msgs = df_msgs.rename(columns={\n",
    "        \"iId\": \"message_id\",\n",
    "        \"iProgressId\": \"progress_id\",\n",
    "        \"iMessageFatherId\": \"answer_to_message_id\",\n",
    "        \"clean_text\": \"content\",\n",
    "        \"Field73\": \"author\",\n",
    "        \"dDate\": \"timestamp\",\n",
    "        \"sAttachment\": \"attachment\"\n",
    "    })[[\"message_id\", \"progress_id\", \"answer_to_message_id\", \"author\", \"timestamp\", \"content\", \"attachment\"]]\n",
    "\n",
    "    # Save result\n",
    "    df_msgs.to_csv(OUTPUT_CSV_MSG, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    print(\"Messages cleaning complete. Saved to\", OUTPUT_CSV_MSG)\n",
    "    print(df_msgs[[\"message_id\", \"progress_id\", \"author\", \"timestamp\", \"content\"]].head())\n",
    "\n",
    "try:\n",
    "    df_progress = pd.read_csv(OUTPUT_CSV_PROGRESS)\n",
    "    print(\"Progress CSV already exists. Skipping cleaning.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Progress CSV not found. Proceeding with cleaning.\")\n",
    "    df_progress = pd.read_csv(INPUT_CSV_PROGRESS)\n",
    "\n",
    "    df_progress[\"clean_description\"] = df_progress[\"sDescription\"].apply(clean_html)\n",
    "\n",
    "    df_progress = df_progress.drop(columns=[\"sDescription\"])\n",
    "\n",
    "    df_progress = df_progress.rename(columns={\n",
    "        \"iId\": \"progress_id\",\n",
    "        \"sTitle\": \"subject\",\n",
    "        \"clean_description\": \"description\",\n",
    "        \"sAuthor\": \"author\",\n",
    "        \"dOpen\": \"created_at\",\n",
    "        \"dClose\": \"closed_at\",\n",
    "        \"dLastChange\": \"updated_at\"\n",
    "    })[[\"progress_id\", \"subject\", \"description\", \"author\", \"created_at\", \"closed_at\", \"updated_at\"]]\n",
    "\n",
    "    df_progress.to_csv(OUTPUT_CSV_PROGRESS, index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    print(\"Progress cleaning complete. Saved to\", OUTPUT_CSV_PROGRESS)\n",
    "    print(df_progress[[\"progress_id\", \"author\", \"created_at\", \"description\"]].head())\n"
   ],
   "id": "5fa9cb48bb3f8b14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create JSON dataset",
   "id": "5087041562f3637a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# If JSON dataset already exists, skip processing\n",
    "\n",
    "try:\n",
    "    with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        dataset = json.load(f)\n",
    "    print(\"JSON dataset already exists. Skipping processing.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"JSON dataset not found. Proceeding with processing.\")\n",
    "    # --- GROUP MESSAGES BY PROGRESS ID ---\n",
    "    grouped = df_msgs.groupby(\"progress_id\")\n",
    "\n",
    "    print(grouped.size().describe())\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    num_of_removed = 0\n",
    "    skipped_progress_empty_metadata = 0\n",
    "\n",
    "    for progress_id, group in grouped:\n",
    "        messages = group.sort_values(\"timestamp\").to_dict(orient=\"records\")\n",
    "        texts = [msg[\"content\"] for msg in messages]\n",
    "\n",
    "        # TF-IDF deduplication\n",
    "        if len(texts) > 1:\n",
    "            tfidf = TfidfVectorizer().fit_transform(texts)\n",
    "            sim_matrix = cosine_similarity(tfidf)\n",
    "            to_remove = set()\n",
    "            for i in range(len(messages)):\n",
    "                for j in range(i + 1, len(messages)):\n",
    "                    if sim_matrix[i, j] >= SIMILARITY_THRESHOLD:\n",
    "                        to_remove.add(j)  # keep earlier msg (i), discard later (j)\n",
    "            num_of_removed += len(to_remove)\n",
    "            messages = [msg for idx, msg in enumerate(messages) if idx not in to_remove]\n",
    "\n",
    "        # Metadata\n",
    "        unique_authors = set(msg[\"author\"] for msg in messages)\n",
    "        total_length = sum(len(msg[\"content\"]) for msg in messages)\n",
    "        total_words = sum(count_words(msg[\"content\"]) for msg in messages)\n",
    "\n",
    "        progress_info = df_progress[df_progress[\"progress_id\"] == progress_id]\n",
    "        if progress_info.empty:\n",
    "            skipped_progress_empty_metadata += 1\n",
    "            continue\n",
    "        meta = progress_info.iloc[0].to_dict()\n",
    "\n",
    "        dataset.append({\n",
    "            \"progress_id\": progress_id,\n",
    "            \"subject\": meta[\"subject\"],\n",
    "            \"description\": meta[\"description\"],\n",
    "            \"created_at\": meta[\"created_at\"],\n",
    "            \"closed_at\": meta[\"closed_at\"],\n",
    "            \"updated_at\": meta[\"updated_at\"],\n",
    "            \"author\": meta[\"author\"],\n",
    "            \"message_count\": len(messages),\n",
    "            \"total_char_length\": total_length,\n",
    "            \"total_words\": total_words,\n",
    "            \"distinct_authors\": len(unique_authors),\n",
    "            \"messages\": messages\n",
    "        })\n",
    "\n",
    "    print(f\"Processed {len(dataset)} progress threads and {len(df_msgs)} messages with {num_of_removed} duplicates removed.\")\n",
    "    print(f\"Removed {skipped_progress_empty_metadata} progress threads present in messages but missing metadata.\")\n",
    "\n",
    "    # Remove progress threads with no messages\n",
    "\n",
    "    dataset = [p for p in dataset if p[\"message_count\"] > 0]\n",
    "\n",
    "    # --- SAVE OUTPUT ---\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        json.dump(dataset, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved structured JSON to {OUTPUT_JSON}\")\n"
   ],
   "id": "925db85aedb8aa79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DATA ANALYSIS",
   "id": "97896cc42c8020be"
  },
  {
   "cell_type": "code",
   "id": "5f2ecedc",
   "metadata": {},
   "source": [
    "# Load structured JSON\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "df_progress_analysis = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"]\n",
    "} for p in dataset])\n",
    "\n",
    "df_progress_analysis.head()\n",
    "\n",
    "# Flatten messages for DataFrame-like manipulation\n",
    "\n",
    "flat_messages = []\n",
    "for prog in dataset:\n",
    "    for msg in prog[\"messages\"]:\n",
    "        flat_messages.append({\n",
    "            \"progress_id\": prog[\"progress_id\"],\n",
    "            \"progress_title\": prog[\"subject\"],\n",
    "            \"progress_created_at\": prog[\"created_at\"],\n",
    "            \"message_id\": msg[\"message_id\"],\n",
    "            \"timestamp\": msg[\"timestamp\"],\n",
    "            \"author\": msg[\"author\"],\n",
    "            \"content\": msg[\"content\"]\n",
    "        })\n",
    "\n",
    "df_messages_analysis = pd.DataFrame(flat_messages)\n",
    "df_messages_analysis.head()\n",
    "\n",
    "# Convert dates to datetime\n",
    "df_progress_analysis[\"created_at\"] = pd.to_datetime(df_progress_analysis[\"created_at\"], errors=\"coerce\")\n",
    "df_progress_analysis[\"closed_at\"] = pd.to_datetime(df_progress_analysis[\"closed_at\"], errors=\"coerce\")\n",
    "df_progress_analysis[\"updated_at\"] = pd.to_datetime(df_progress_analysis[\"updated_at\"], errors=\"coerce\")\n",
    "\n",
    "df_messages_analysis[\"timestamp\"] = pd.to_datetime(df_messages_analysis[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Basic info\n",
    "print(\"Messages shape:\", df_messages_analysis.shape)\n",
    "print(\"Progress shape:\", df_progress_analysis.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic statistics",
   "id": "bb757f1ed0613f95"
  },
  {
   "cell_type": "code",
   "id": "c6de31ce",
   "metadata": {},
   "source": [
    "# Number of unique progress threads\n",
    "n_threads = df_progress_analysis.shape[0]\n",
    "print(\"Unique progress threads:\", n_threads)\n",
    "\n",
    "# Messages per thread\n",
    "messages_per_thread = df_progress_analysis[\"message_count\"]\n",
    "print(\"Messages per thread (summary):\")\n",
    "print(messages_per_thread.describe())\n",
    "\n",
    "# Plot num of messages in log scale because of skewed distribution\n",
    "sns.histplot(np.log(messages_per_thread), bins=50, kde=False)\n",
    "plt.title(\"Messages per Progress Thread\")\n",
    "plt.xlabel(\"log(Messages)\")\n",
    "plt.ylabel(\"Number of Threads\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bca9da6",
   "metadata": {},
   "source": [
    "# Message lengths\n",
    "df_messages_analysis[\"msg_length\"] = df_messages_analysis[\"content\"].str.len()\n",
    "print(\"Message length (chars) summary:\")\n",
    "print(df_messages_analysis[\"msg_length\"].describe())\n",
    "\n",
    "# Plot message lengths in log scale because of skewed distribution\n",
    "sns.histplot(np.log(df_messages_analysis[\"msg_length\"]), bins=50, kde=True)\n",
    "plt.title(\"Log-Scaled Distribution of Message Lengths\")\n",
    "plt.xlabel(\"log(Characters)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Number of unique authors\n",
    "n_authors = df_messages_analysis[\"author\"].nunique()\n",
    "print(\"Unique authors:\", n_authors)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c03cc1ca",
   "metadata": {},
   "source": [
    "# Thread durations (min → max message timestamp per thread)\n",
    "thread_times = df_messages_analysis.groupby(\"progress_id\")[\"timestamp\"].agg([\"min\", \"max\"])\n",
    "thread_times[\"duration_days\"] = (thread_times[\"max\"] - thread_times[\"min\"]).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "# Plot\n",
    "sns.histplot(thread_times[\"duration_days\"], bins=50, kde=False)\n",
    "plt.title(\"Progress Thread Durations\")\n",
    "plt.xlabel(\"Duration (days)\")\n",
    "plt.ylabel(\"Number of Threads\")\n",
    "plt.show()\n",
    "\n",
    "thread_times[\"duration_days\"].describe()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG",
   "id": "31d8f21432030d5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load and process messages for RAG",
   "id": "48da11100d4ca89a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- LOAD AND FLATTEN JSON WITH CONTEXT ---\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "flat_messages = []\n",
    "for progress in dataset:\n",
    "    progress_subject = progress.get(\"subject\", \"\")\n",
    "    progress_description = str(progress.get(\"description\", \"\") or \"\")\n",
    "    for msg in progress[\"messages\"]:\n",
    "        enriched_content = (\n",
    "            f\"[Progress Title: {progress_subject}]\\n\\n\"\n",
    "            f\"[Progress Description: {progress_description.strip()}]\\n\\n\"\n",
    "            f\"{msg['content'].strip()}\"\n",
    "        )\n",
    "        flat_messages.append({\n",
    "            \"progress_id\": progress[\"progress_id\"],\n",
    "            \"progress_title\": progress_subject,\n",
    "            \"progress_description\": progress_description,\n",
    "            \"message_id\": msg[\"message_id\"],\n",
    "            \"timestamp\": msg[\"timestamp\"],\n",
    "            \"author\": msg[\"author\"],\n",
    "            \"original_content\": msg[\"content\"],\n",
    "            \"enriched_content\": enriched_content\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(flat_messages)\n",
    "df = df.dropna(subset=[\"enriched_content\"])\n",
    "df = df[df[\"enriched_content\"].str.len() > 10].reset_index(drop=True)\n",
    "\n",
    "# --- EMBEDDINGS ---\n",
    "\n",
    "model = None\n",
    "embeddings = None\n",
    "\n",
    "for model_name in EMBEDDING_MODELS:\n",
    "    index_path = INDEX_PATH + \"_\" + model_name.replace(\"/\", \"_\")\n",
    "    metadata_path = METADATA_PATH + \"_\" + model_name.replace(\"/\", \"_\")\n",
    "\n",
    "    # If index and metadata already exist, skip processing\n",
    "\n",
    "    if faiss.read_index(INDEX_PATH + \"_\" + model_name.replace(\"/\", \"_\") + \".index\") and \\\n",
    "            pd.io.json.read_json(METADATA_PATH + \"_\" + model_name.replace(\"/\", \"_\") + \".json\").shape[0] > 0:\n",
    "        print(f\"Index and metadata for {model_name} already exist. Skipping processing.\")\n",
    "        continue\n",
    "\n",
    "    print(\"Loading embedding model:\", model_name)\n",
    "    model = SentenceTransformer(model_name, trust_remote_code=True)\n",
    "    print(\"Encoding enriched messages...\")\n",
    "    embeddings = model.encode(df[\"enriched_content\"].tolist(), show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    # --- FAISS INDEXING ---\n",
    "    print(\"Building FAISS index...\")\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, index_path + \".index\")\n",
    "    print(f\"FAISS index saved to {index_path}.index\")\n",
    "\n",
    "    # Save metadata\n",
    "    df.to_json(metadata_path + \".json\", orient=\"records\", force_ascii=False, indent=2)\n",
    "    print(f\"Metadata saved to {metadata_path}.json\")\n",
    "\n",
    "# --- Unload model and free GPU memory ---\n",
    "\n",
    "if model is not None:\n",
    "    del model\n",
    "    del embeddings\n",
    "    del index\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ],
   "id": "dabb2a68a2d59cde",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load and test FAISS index",
   "id": "97d279b7f2a2b80c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_index_and_metadata(model_name):\n",
    "    idx_path = f\"{INDEX_PATH}_{model_name.replace('/', '_')}.index\"\n",
    "    meta_path = f\"{METADATA_PATH}_{model_name.replace('/', '_')}.json\"\n",
    "\n",
    "    index = faiss.read_index(idx_path)\n",
    "    df = pd.read_json(meta_path)\n",
    "    return index, df\n",
    "\n",
    "def retrieve_top_k(query, model, index, df, k=5):\n",
    "    q_emb = model.encode([query])[0].astype(\"float32\").reshape(1, -1)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return df.iloc[I[0]][[\"progress_title\", \"timestamp\", \"author\", \"original_content\"]].to_dict(orient=\"records\")"
   ],
   "id": "65c5ce6d88d12ab9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Test retrieval with different models -> add test set",
   "id": "66eae8fa6151869d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for model_name in EMBEDDING_MODELS:\n",
    "#     print(f\"Testing model: {model_name}\")\n",
    "#     index, df = load_index_and_metadata(model_name)\n",
    "#\n",
    "#     # Test query\n",
    "#     query = \"cruscotto mim?\"\n",
    "#     results = retrieve_top_k(query, SentenceTransformer(model_name, trust_remote_code=True), index, df, k=5)\n",
    "#\n",
    "#     print(f\"Top 5 results for query '{query}':\")\n",
    "#     for res in results:\n",
    "#         print(f\"- {res['progress_title']} by {res['author']} at {res['timestamp']}: {res['original_content'][:100]}...\")  # Show first 100 chars"
   ],
   "id": "151e3234890c5894",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Summaries generation",
   "id": "91307dbc54df9beb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### FINDING SIGNIFICANT PROGRESS THREADS",
   "id": "2838231bd3a461a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Load structured JSON\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "# Extract 4 short progress threads, 4 medium size, and 4 long ones\n",
    "\n",
    "short_threads = df_dataset[(df_dataset[\"total_words\"] >= 300) & (df_dataset[\"total_words\"] < 3000)].sample(n=4, random_state=8000)\n",
    "\n",
    "medium_threads = df_dataset[(df_dataset[\"total_words\"] >= 3000) & (df_dataset[\"total_words\"] < 6000)].sample(n=4, random_state=8000)\n",
    "\n",
    "long_threads = df_dataset[df_dataset[\"total_words\"] >= 6000].sample(n=4, random_state=8000)\n",
    "\n",
    "print(\"Short threads (less than 2000 words):\")\n",
    "print(short_threads[[\"progress_id\", \"subject\", \"total_char_length\", \"total_words\"]])\n",
    "print(\"\\nMedium threads (2000 to 6000 words):\")\n",
    "print(medium_threads[[\"progress_id\", \"subject\", \"total_char_length\", \"total_words\"]])\n",
    "print(\"\\nLong threads (more than 6000 words):\")\n",
    "print(long_threads[[\"progress_id\", \"subject\", \"total_char_length\", \"total_words\"]])"
   ],
   "id": "e4b7c33b94bed38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### DIRECT SUMMARIES",
   "id": "3d982c4e48f8eb98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the progress IDs for which you want to generate summaries\n",
    "progress_ids_for_test_short = short_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_medium = medium_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_long = long_threads[\"progress_id\"].tolist()\n",
    "\n",
    "progress_ids_for_test  = progress_ids_for_test_short\n",
    "progress_ids_for_test += progress_ids_for_test_medium\n",
    "progress_ids_for_test += progress_ids_for_test_long\n",
    "\n",
    "# Load structured JSON\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "llm_models = [\n",
    "    \"ollama\",\n",
    "    \"gemini\",\n",
    "    \"gpt\"\n",
    "]\n",
    "\n",
    "for progress_id in progress_ids_for_test:\n",
    "    print(f\"Generating summary for progress ID: {progress_id}\")\n",
    "\n",
    "    row = df_dataset[df_dataset[\"progress_id\"] == progress_id]\n",
    "    if row.empty:\n",
    "        print(f\"No data found for progress ID {progress_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for model in llm_models:\n",
    "        if os.path.exists(f\"DATA/SUMMARIES/{model}/DIRECT/summary_progress_{progress_id}.txt\"):\n",
    "            print(f\"Summary for progress ID {progress_id} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        progress_data = row.iloc[0].to_dict()\n",
    "        time_of_start = datetime.datetime.now()\n",
    "        summary = utils.summarize_progress_direct(progress_data, llm=model)\n",
    "        time_of_end = datetime.datetime.now()\n",
    "\n",
    "        summary += f\"\\n\\n---\\nOriginal length: {progress_data['total_char_length']} characters and {progress_data['total_words']} words.\"\n",
    "        summary += f\"\\nSummary length: {len(summary)} characters and {count_words(summary)} words.\"\n",
    "\n",
    "        compression_ratio = round(len(summary) / progress_data[\"total_char_length\"], 2)\n",
    "        summary += f\"\\nCompression ratio: {compression_ratio} (summary length / original length).\"\n",
    "\n",
    "        summary += f\"\\nEstimated input tokens: {progress_data['total_char_length'] // 4}.\"\n",
    "        summary += f\"\\nEstimated output tokens: {len(summary) // 4}.\"\n",
    "\n",
    "        summary_time = time_of_end - time_of_start\n",
    "        summary += f\"\\nSummary generated in {summary_time.total_seconds()} seconds.\"\n",
    "        # print(f\"Summary for progress ID {progress_id}:\\n{summary}\\n\")\n",
    "\n",
    "        # Save summary in txt file\n",
    "\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/DIRECT\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        with open(f\"{dir_path}/summary_progress_{progress_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary saved to {dir_path}/summary_progress_{progress_id}.txt\\n\")"
   ],
   "id": "7d4049328f073f48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PIPELINE IDEA 1\n",
    "- If progress length is short enough → single summary\n",
    "- If progress length is long → multiple summaries → make final summary from them"
   ],
   "id": "f5f4a8b051357ab8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the progress IDs for which you want to generate summaries\n",
    "progress_ids_for_test_short = short_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_medium = medium_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_long = long_threads[\"progress_id\"].tolist()\n",
    "\n",
    "progress_ids_for_test  = progress_ids_for_test_short\n",
    "progress_ids_for_test += progress_ids_for_test_medium\n",
    "progress_ids_for_test += progress_ids_for_test_long\n",
    "\n",
    "# Load structured JSON\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "llm_models = [\n",
    "    \"ollama\",\n",
    "    \"gemini\",\n",
    "    \"gpt\"\n",
    "]\n",
    "\n",
    "for progress_id in progress_ids_for_test:\n",
    "    print(f\"Generating summary for progress ID: {progress_id}\")\n",
    "\n",
    "    row = df_dataset[df_dataset[\"progress_id\"] == progress_id]\n",
    "    if row.empty:\n",
    "        print(f\"No data found for progress ID {progress_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for model in llm_models:\n",
    "        if os.path.exists(f\"DATA/SUMMARIES/{model}/IDEA_1/summary_progress_{progress_id}.txt\"):\n",
    "            print(f\"Summary for progress ID {progress_id} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        progress_data = row.iloc[0].to_dict()\n",
    "        time_of_start = datetime.datetime.now()\n",
    "        summary = utils.summarize_progress_idea_1(progress_data, llm=model)\n",
    "        time_of_end = datetime.datetime.now()\n",
    "\n",
    "        summary += f\"\\n\\n---\\nOriginal length: {progress_data['total_char_length']} characters and {progress_data['total_words']} words.\"\n",
    "        summary += f\"\\nSummary length: {len(summary)} characters and {count_words(summary)} words.\"\n",
    "\n",
    "        compression_ratio = round(len(summary) / progress_data[\"total_char_length\"], 2)\n",
    "        summary += f\"\\nCompression ratio: {compression_ratio} (summary length / original length).\"\n",
    "\n",
    "        summary += f\"\\nEstimated input tokens: {progress_data['total_char_length'] // 4}.\"\n",
    "        summary += f\"\\nEstimated output tokens: {len(summary) // 4}.\"\n",
    "\n",
    "        summary_time = time_of_end - time_of_start\n",
    "        summary += f\"\\nSummary generated in {summary_time.total_seconds()} seconds.\"\n",
    "        # print(f\"Summary for progress ID {progress_id}:\\n{summary}\\n\")\n",
    "\n",
    "        # Save summary in txt file\n",
    "\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/IDEA_1\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        with open(f\"{dir_path}/summary_progress_{progress_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary saved to {dir_path}/summary_progress_{progress_id}.txt\\n\")"
   ],
   "id": "fd35cea0e95087e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PIPELINE IDEA 2\n",
    "Organize progress threads into groups (annual, semi-annual, etc.) and summarize each group. Then union the summaries while maintaining chronological order."
   ],
   "id": "6956f0eaaa320e95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the progress IDs for which you want to generate summaries\n",
    "progress_ids_for_test_short = short_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_medium = medium_threads[\"progress_id\"].tolist()\n",
    "progress_ids_for_test_long = long_threads[\"progress_id\"].tolist()\n",
    "\n",
    "progress_ids_for_test  = progress_ids_for_test_short\n",
    "progress_ids_for_test += progress_ids_for_test_medium\n",
    "progress_ids_for_test += progress_ids_for_test_long\n",
    "\n",
    "# Load structured JSON\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "llm_models = [\n",
    "    \"ollama\",\n",
    "    \"gemini\",\n",
    "    \"gpt\"\n",
    "]\n",
    "\n",
    "for progress_id in progress_ids_for_test:\n",
    "    print(f\"Generating summary for progress ID: {progress_id}\")\n",
    "\n",
    "    row = df_dataset[df_dataset[\"progress_id\"] == progress_id]\n",
    "    if row.empty:\n",
    "        print(f\"No data found for progress ID {progress_id}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    for model in llm_models:\n",
    "        if os.path.exists(f\"DATA/SUMMARIES/{model}/IDEA_2/summary_progress_{progress_id}.txt\"):\n",
    "            print(f\"Summary for progress ID {progress_id} already exists. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        progress_data = row.iloc[0].to_dict()\n",
    "        time_of_start = datetime.datetime.now()\n",
    "        summary = utils.summarize_progress_idea_2(progress_data, llm=model)\n",
    "        time_of_end = datetime.datetime.now()\n",
    "        summary += f\"\\n\\n---\\nOriginal length: {progress_data['total_char_length']} characters and {progress_data['total_words']} words.\"\n",
    "        summary += f\"\\nSummary length: {len(summary)} characters and {count_words(summary)} words.\"\n",
    "\n",
    "        compression_ratio = round(len(summary) / progress_data[\"total_char_length\"], 2)\n",
    "        summary += f\"\\nCompression ratio: {compression_ratio} (summary length / original length).\"\n",
    "\n",
    "        summary += f\"\\nEstimated input tokens: {progress_data['total_char_length'] // 4}.\"\n",
    "        summary += f\"\\nEstimated output tokens: {len(summary) // 4}.\"\n",
    "\n",
    "        summary_time = time_of_end - time_of_start\n",
    "        summary += f\"\\nSummary generated in {summary_time.total_seconds()} seconds.\"\n",
    "        # print(f\"Summary for progress ID {progress_id}:\\n{summary}\\n\")\n",
    "\n",
    "        # Save summary in txt file\n",
    "\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/IDEA_2\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "        with open(f\"{dir_path}/summary_progress_{progress_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(summary)\n",
    "        print(f\"Summary saved to {dir_path}/summary_progress_{progress_id}.txt\\n\")"
   ],
   "id": "8c61fc11b5fc2129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# VALIDATION",
   "id": "2b6d17643913ca10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SUMMAC",
   "id": "893e9e9988e6b90c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load JSON dataset\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# DataFrame for easier manipulation\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "llm_models = [\n",
    "    \"ollama\",\n",
    "    \"gemini\",\n",
    "    \"gpt\"\n",
    "]\n",
    "\n",
    "if not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_direct.csv\"):\n",
    "    summaries = []\n",
    "    for model in llm_models:\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/DIRECT\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"No summaries found for model {model}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_path, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    summaries.append({\n",
    "                        \"model\": model,\n",
    "                        \"mode\": \"DIRECT\",\n",
    "                        \"file_name\": file_name,\n",
    "                        \"content\": f.read(),\n",
    "                        \"progress_id\": int(file_name.split(\"_\")[2].split(\".\")[0]),\n",
    "                        \"summac_score\": None,  # Placeholder for SUMMAC score\n",
    "                        \"summac_score_with_contradiction\": None,  # Placeholder for SUMMAC score\n",
    "                        \"qags_score\": None\n",
    "                    })\n",
    "\n",
    "    for summary in summaries:\n",
    "        summac, summac_with_c = utils.validate_summary_with_nli_hybrid(summary[\"content\"], df_dataset[df_dataset[\"progress_id\"] == summary[\"progress_id\"]].iloc[0].to_dict())\n",
    "        print(f\"Validation result for {summary['file_name']}: {summac}, {summac_with_c}\")\n",
    "        summary[\"summac_score\"] = summac\n",
    "        summary[\"summac_score_with_contradiction\"] = summac_with_c\n",
    "\n",
    "    # Save results to a DataFrame\n",
    "\n",
    "    df_summaries = pd.DataFrame(summaries)\n",
    "\n",
    "    # Save to CSV\n",
    "\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_direct.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "if not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_1.csv\"):\n",
    "    summaries = []\n",
    "    for model in llm_models:\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/IDEA_1\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"No summaries found for model {model}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_path, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    summaries.append({\n",
    "                        \"model\": model,\n",
    "                        \"mode\": \"IDEA_1\",\n",
    "                        \"file_name\": file_name,\n",
    "                        \"content\": f.read(),\n",
    "                        \"progress_id\": int(file_name.split(\"_\")[2].split(\".\")[0]),\n",
    "                        \"summac_score\": None,  # Placeholder for SUMMAC score\n",
    "                        \"summac_score_with_contradiction\": None,  # Placeholder for SUMMAC score\n",
    "                        \"qags_score\": None\n",
    "                    })\n",
    "\n",
    "    for summary in summaries:\n",
    "        summac, summac_with_c = utils.validate_summary_with_nli_hybrid(summary[\"content\"], df_dataset[df_dataset[\"progress_id\"] == summary[\"progress_id\"]].iloc[0].to_dict())\n",
    "        print(f\"Validation result for {summary['file_name']}: {summac}, {summac_with_c}\")\n",
    "        summary[\"summac_score\"] = summac\n",
    "        summary[\"summac_score_with_contradiction\"] = summac_with_c\n",
    "\n",
    "    # Save results to a DataFrame\n",
    "\n",
    "    df_summaries = pd.DataFrame(summaries)\n",
    "\n",
    "    # Save to CSV\n",
    "\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_1.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "if not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_2.csv\"):\n",
    "    summaries = []\n",
    "    for model in llm_models:\n",
    "        dir_path = f\"DATA/SUMMARIES/{model}/IDEA_2\"\n",
    "        if not os.path.exists(dir_path):\n",
    "            print(f\"No summaries found for model {model}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_path, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "                    summaries.append({\n",
    "                        \"model\": model,\n",
    "                        \"mode\": \"IDEA_2\",\n",
    "                        \"file_name\": file_name,\n",
    "                        \"content\": f.read(),\n",
    "                        \"progress_id\": int(file_name.split(\"_\")[2].split(\".\")[0]),\n",
    "                        \"summac_score\": None,  # Placeholder for SUMMAC score\n",
    "                        \"summac_score_with_contradiction\": None,  # Placeholder for SUMMAC score\n",
    "                        \"qags_score\": None\n",
    "                    })\n",
    "\n",
    "    for summary in summaries:\n",
    "        summac, summac_with_c = utils.validate_summary_with_nli_hybrid(summary[\"content\"], df_dataset[df_dataset[\"progress_id\"] == summary[\"progress_id\"]].iloc[0].to_dict())\n",
    "        print(f\"Validation result for {summary['file_name']}: {summac}, {summac_with_c}\")\n",
    "        summary[\"summac_score\"] = summac\n",
    "        summary[\"summac_score_with_contradiction\"] = summac_with_c\n",
    "\n",
    "    # Save results to a DataFrame\n",
    "\n",
    "    df_summaries = pd.DataFrame(summaries)\n",
    "\n",
    "    # Save to CSV\n",
    "\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_2.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n"
   ],
   "id": "ebc20061bada5df2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### QAGS",
   "id": "5adedd215b9de279"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load JSON dataset\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# DataFrame for easier manipulation\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "llm_models = [\n",
    "    \"ollama\",\n",
    "    \"gemini\",\n",
    "    \"gpt\"\n",
    "]\n",
    "\n",
    "# Ottengo il DataFrame con i risultati delle validazioni SUMMAC\n",
    "df_summaries = pd.DataFrame()\n",
    "if os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_direct.csv\"):\n",
    "    df_summaries = pd.concat([df_summaries, pd.read_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_direct.csv\", encoding=\"utf-8\")], ignore_index=True)\n",
    "if os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_1.csv\"):\n",
    "    df_summaries = pd.concat([df_summaries, pd.read_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_1.csv\", encoding=\"utf-8\")], ignore_index=True)\n",
    "if os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_2.csv\"):\n",
    "    df_summaries = pd.concat([df_summaries, pd.read_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_2.csv\", encoding=\"utf-8\")], ignore_index=True)\n",
    "\n",
    "if df_summaries.empty:\n",
    "    print(\"No validation results found. Please run the SUMMAC validation first.\")\n",
    "elif not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_qags.csv\"):\n",
    "    # Calcolo il QAGS score per ogni sommario\n",
    "    df_summaries[\"qags_score\"] = df_summaries.apply(lambda row: utils.validate_summary_with_qags_batched(row[\"content\"], df_dataset[df_dataset[\"progress_id\"] == row[\"progress_id\"]].iloc[0].to_dict()), axis=1)\n",
    "\n",
    "    # Salvo i risultati in un nuovo file CSV\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_qags.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "    print(\"QAGS validation results saved to DATA/SUMMARIES/TEST/summaries_validation_results_qags.csv\")"
   ],
   "id": "7b73afcad6dd2a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GPT",
   "id": "42658a972e01972c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load JSON dataset\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# DataFrame for easier manipulation\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "# Ottengo il DataFrame con i risultati delle validazioni SUMMAC\n",
    "df_summaries = pd.DataFrame()\n",
    "if os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_qags.csv\"):\n",
    "    df_summaries = pd.concat([df_summaries, pd.read_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_qags.csv\", encoding=\"utf-8\")], ignore_index=True)\n",
    "\n",
    "if df_summaries.empty:\n",
    "    print(\"No validation results found. Please run the SUMMAC validation first.\")\n",
    "elif not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_gpt.csv\"):\n",
    "    # Calcolo il QAGS score per ogni sommario\n",
    "    df_summaries[\"gpt_score\"] = df_summaries.apply(lambda row: utils.gpt_score_summary(row[\"content\"], df_dataset[df_dataset[\"progress_id\"] == row[\"progress_id\"]].iloc[0].to_dict()), axis=1)\n",
    "\n",
    "    # Salvo i risultati in un nuovo file CSV\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_gpt.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "    print(\"QAGS validation results saved to DATA/SUMMARIES/TEST/summaries_validation_results_gpt.csv\")"
   ],
   "id": "fbb54315630d856e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### GEMINI",
   "id": "fba7da68ca45edce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load JSON dataset\n",
    "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# DataFrame for easier manipulation\n",
    "df_dataset = pd.DataFrame([{\n",
    "    \"progress_id\": p[\"progress_id\"],\n",
    "    \"subject\": p[\"subject\"],\n",
    "    \"description\": p.get(\"description\", \"\"),\n",
    "    \"created_at\": p[\"created_at\"],\n",
    "    \"closed_at\": p.get(\"closed_at\", None),\n",
    "    \"updated_at\": p[\"updated_at\"],\n",
    "    \"author\": p[\"author\"],\n",
    "    \"message_count\": p[\"message_count\"],\n",
    "    \"total_char_length\": p[\"total_char_length\"],\n",
    "    \"total_words\": p[\"total_words\"],\n",
    "    \"distinct_authors\": p[\"distinct_authors\"],\n",
    "    \"messages\": p[\"messages\"]\n",
    "} for p in dataset])\n",
    "\n",
    "# Ottengo il DataFrame con i risultati delle validazioni SUMMAC\n",
    "df_summaries = pd.DataFrame()\n",
    "if os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_gpt.csv\"):\n",
    "    df_summaries = pd.concat([df_summaries, pd.read_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_gpt.csv\", encoding=\"utf-8\")], ignore_index=True)\n",
    "\n",
    "if df_summaries.empty:\n",
    "    print(\"No validation results found. Please run the SUMMAC validation first.\")\n",
    "elif not os.path.exists(\"DATA/SUMMARIES/TEST/summaries_validation_results_gemini.csv\"):\n",
    "    # Calcolo il QAGS score per ogni sommario\n",
    "    df_summaries[\"gemini_score\"] = df_summaries.apply(lambda row: utils.gemini_score_summary(row[\"content\"], df_dataset[df_dataset[\"progress_id\"] == row[\"progress_id\"]].iloc[0].to_dict()), axis=1)\n",
    "\n",
    "    # Salvo i risultati in un nuovo file CSV\n",
    "    df_summaries.to_csv(\"DATA/SUMMARIES/TEST/summaries_validation_results_gemini.csv\", index=False, encoding=\"utf-8\", errors=\"replace\")\n",
    "    print(\"QAGS validation results saved to DATA/SUMMARIES/TEST/summaries_validation_results_gemini.csv\")"
   ],
   "id": "e59f0655166128c8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
